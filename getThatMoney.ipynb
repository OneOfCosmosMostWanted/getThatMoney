{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOmbrclwq3asl8lFhkVALEk"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n"
      ],
      "metadata": {
        "id": "WPC8a54vVtam"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ZE5es-_TAIY"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Check available GPUs\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
        "\n",
        "# Configure GPU settings\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)  # Allow memory growth\n",
        "    except RuntimeError as e:\n",
        "        print(e)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import yfinance as yf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Hyperparameters\n",
        "input_size = 1    # Only using the \"Close\" price for prediction\n",
        "sequence_length = 60  # Using 60 previous days to predict the next days\n",
        "target_sequences = [5, 15, 30]   # Predicting the next days\n",
        "batch_size = 64\n",
        "num_epochs = 50\n",
        "\n",
        "\n",
        "# Downloading stock data from Yahoo Finance\n",
        "# output: 2D array with a single column, (num, 1)\n",
        "def get_stock_data(ticker):\n",
        "    df = yf.download(ticker, start=\"2010-01-01\", end=\"2023-01-01\")\n",
        "    return df['Close'].values.reshape(-1, 1)\n",
        "\n",
        "# Preprocessing the data\n",
        "def preprocess_data(data, sequence_length, target_sequences):\n",
        "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "    data = scaler.fit_transform(data)\n",
        "\n",
        "    X, y = [], []\n",
        "\n",
        "    set_zero_second = False\n",
        "    set_zero_third = False\n",
        "    # set the targets until the smallest target sequnece hits the end\n",
        "    for i in range(len(data) - sequence_length - target_sequences[0] + 1):\n",
        "        if not set_zero_second:\n",
        "            if i + sequence_length + target_sequences[1] > len(data):\n",
        "                set_zero_second = True\n",
        "        if not set_zero_third:\n",
        "            if i + sequence_length + target_sequences[2] > len(data):\n",
        "                set_zero_third = True\n",
        "        X.append(data[i:i+sequence_length])\n",
        "        scales = []\n",
        "        for scale, output_size in enumerate(target_sequences):\n",
        "            if scale == 1 and set_zero_second:\n",
        "                scales.append(np.zeros((output_size, 1)))\n",
        "            elif scale == 2 and set_zero_third:\n",
        "                scales.append(np.zeros((output_size, 1)))\n",
        "            else:\n",
        "                scales.append(data[i+sequence_length:i+sequence_length+output_size])\n",
        "        scales_flat = [np.ravel(target) for target in scales]\n",
        "        y.append(scales_flat)\n",
        "\n",
        "    X = np.array(X)\n",
        "    y = np.array(y, dtype=object)\n",
        "\n",
        "    X = X.reshape((X.shape[0], X.shape[1], 1))\n",
        "    y = np.array([np.concatenate(s) for s in y])\n",
        "    return X, y, scaler\n",
        "\n",
        "def split_data(X, y, train_size):\n",
        "    split = int(0.8 * len(X))\n",
        "    X_train, y_train = X[:train_size], y[:train_size]\n",
        "    X_test, y_test = X[train_size:], y[train_size:]\n",
        "\n",
        "    return X_train, y_train, X_test, y_test\n",
        "\n"
      ],
      "metadata": {
        "id": "fswdd6XTT-zZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = get_stock_data('IBM')\n",
        "X, y, scaler = preprocess_data(data, sequence_length, target_sequences)\n",
        "X_train, y_train, X_test, y_test = split_data(X, y, int(0.8 * len(X)))"
      ],
      "metadata": {
        "id": "S3DvA98PbmRk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Architecture configuration\n",
        "\n",
        "from keras.layers import LSTM, Dropout, Dense\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model\n",
        "from keras.models import Sequential\n",
        "\n",
        "\"\"\"\n",
        "() = LSTM layer (features, return_sequences, skip_connection)\n",
        "S = prediction layer\n",
        "C = concatenation\n",
        "\"\"\"\n",
        "config = [\n",
        "    (128, True, False),\n",
        "    (128, True, True),\n",
        "    (64, True, False),\n",
        "    (64, True, True),\n",
        "    (32, True, False),\n",
        "    (32, True, False), # To this point is LSTMs (time_series interpretion)\n",
        "    \"S\",\n",
        "    (64, True, False),\n",
        "    \"C\",\n",
        "    (64, True, False),\n",
        "    \"S\",\n",
        "    (128, True, False),\n",
        "    \"C\",\n",
        "    (128, True, False),\n",
        "    \"S\",\n",
        "]"
      ],
      "metadata": {
        "id": "b1TDO-Qwbpx8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ScalePrediction(tf.keras.layers.Layer):\n",
        "    def __init__(self, features, target_sequence):\n",
        "        super(ScalePrediction, self).__init__()\n",
        "        self.features = features\n",
        "        self.target_sequence = target_sequence\n",
        "        self.layers = []\n",
        "        self.layers.append(LSTM(features))\n",
        "        self.layers.append(ResidualBlock(features))\n",
        "        self.layers.append(Dense(target_sequence))\n",
        "\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = inputs\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return x\n",
        "\n",
        "class ResidualBlock(tf.keras.layers.Layer):\n",
        "    def __init__(self, features):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.layers = []\n",
        "        self.layers.append(Dense(features, activation='relu'))\n",
        "        self.layers.append(Dense(features, activation='relu'))\n",
        "\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = inputs\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return x + inputs\n",
        "\n",
        "# Output shape: (_, 50)\n",
        "# 50: concatenated output sequence lengths e.g, 5 + 15 + 30\n",
        "class customModel(Model):\n",
        "    def __init__(self, sequence_length):\n",
        "        super(customModel, self).__init__()\n",
        "        self.sequence_length = sequence_length\n",
        "        self.list_of_concats_A = []\n",
        "        self.list_of_concats_B = []\n",
        "        self.layers_list = self._create_layers()\n",
        "\n",
        "    def call(self, inputs):\n",
        "        outputs = []\n",
        "        route_connections = []\n",
        "\n",
        "        x = inputs\n",
        "        for i, layer in enumerate(self.layers_list):\n",
        "            if isinstance(layer, ScalePrediction):\n",
        "                outputs.append(layer(x))\n",
        "            else:\n",
        "                x = layer(x)\n",
        "                if i in self.list_of_concats_A:\n",
        "                    route_connections.append(x)\n",
        "                elif i in self.list_of_concats_B:\n",
        "                    x = tf.concat([x, route_connections.pop()], axis=-1)\n",
        "        # Concatenate all tensors along the last axis\n",
        "        concatenated_outputs = tf.concat(outputs, axis=-1)\n",
        "        flattened_outputs = tf.reshape(concatenated_outputs, [-1])\n",
        "        return concatenated_outputs\n",
        "\n",
        "    def _create_layers(self):\n",
        "        layers = []\n",
        "        count = 0\n",
        "        for i, module in enumerate(config):\n",
        "            if isinstance(module, tuple):\n",
        "                if i == 0:\n",
        "                    layers.append(LSTM(module[0], return_sequences=module[1], input_shape=(X_train.shape[1], X_train.shape[2])))\n",
        "                else:\n",
        "                    layers.append(LSTM(module[0], return_sequences=module[1]))\n",
        "                if module[2]:\n",
        "                    self.list_of_concats_A.append(i)\n",
        "            elif module == \"S\":\n",
        "                prev_features = config[i-1][0]\n",
        "                layers.append(ScalePrediction(prev_features, target_sequences[count]))\n",
        "                count += 1\n",
        "\n",
        "            elif module == \"C\":\n",
        "                self.list_of_concats_B.append(i-1)\n",
        "\n",
        "        return layers"
      ],
      "metadata": {
        "id": "KiIXE3z1cHOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def custom_loss(y_true, y_pred):\n",
        "#     loss = 0.0\n",
        "#     batch_size = tf.shape(y_true)[0]\n",
        "#     # Check shapes\n",
        "#     tf.print(\"y_true shape:\", tf.shape(y_true))\n",
        "#     tf.print(\"y_pred shape:\", tf.shape(y_pred))\n",
        "#     for i in range(batch_size):\n",
        "#         # get the predictions of each batch\n",
        "#         y_pred_column = y_pred[i, :]\n",
        "#         y_true_column = y_true[i, :]\n",
        "\n",
        "#         # Only calculate loss for non-zero predictions\n",
        "#         non_zero_mask = tf.not_equal(y_true_column, 0)  # Mask for non-zero values\n",
        "#         y_pred_non_zero = tf.boolean_mask(y_pred_column, non_zero_mask)\n",
        "#         y_true_non_zero = tf.boolean_mask(y_true_column, non_zero_mask)\n",
        "\n",
        "#         # Calculate MSE for non-zero values\n",
        "#         loss += tf.reduce_mean(tf.square(y_pred_non_zero - y_true_non_zero))\n",
        "#     return loss\n",
        "\n",
        "\n",
        "def custom_loss(y_true, y_pred):\n",
        "    # Define a function to compute the loss for each batch\n",
        "    def batch_loss_fn(batch_idx):\n",
        "        # Get the predictions and true values of each batch\n",
        "        y_pred_column = y_pred[batch_idx, :]\n",
        "        y_true_column = y_true[batch_idx, :]\n",
        "\n",
        "        # Only calculate loss for non-zero predictions\n",
        "        non_zero_mask = tf.not_equal(y_true_column, 0)  # Mask for non-zero values\n",
        "        y_pred_non_zero = tf.boolean_mask(y_pred_column, non_zero_mask)\n",
        "        y_true_non_zero = tf.boolean_mask(y_true_column, non_zero_mask)\n",
        "\n",
        "        # Calculate MSE for non-zero values\n",
        "        return tf.reduce_mean(tf.square(y_pred_non_zero - y_true_non_zero))\n",
        "\n",
        "    # Get the number of batches (first dimension)\n",
        "    num_batches = tf.shape(y_true)[0]\n",
        "\n",
        "    # Use tf.map_fn to apply the batch_loss_fn to each batch with fn_output_signature\n",
        "    losses = tf.map_fn(batch_loss_fn, tf.range(num_batches), fn_output_signature=tf.float32)\n",
        "    # Return the sum of losses\n",
        "    return tf.reduce_sum(losses)\n"
      ],
      "metadata": {
        "id": "-AsqL0VbcpsS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = customModel(sequence_length)\n",
        "# (batch_size=None, timesteps=50, features=20)\n",
        "input_shape = (1, 60, 1)\n",
        "dummy_input = tf.random.normal(input_shape)  # Create a dummy input tensor\n",
        "output = model(dummy_input)  # Call the model to build it\n",
        "print(output)\n",
        "# Print the model summary to see the total number of parameters\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "1s4wea4UlGzj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model with multiple outputs\n",
        "model.compile(optimizer='adam', loss=custom_loss)\n",
        "X_train = tf.convert_to_tensor(X_train)\n",
        "y_train = tf.convert_to_tensor(y_train)\n",
        "# Train the model\n",
        "\n",
        "model.fit(X_train, y_train, epochs=num_epochs, batch_size=batch_size, verbose=1)\n"
      ],
      "metadata": {
        "id": "4hPJprPZlkjE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "indices = tf.range(0, len(X_test), 29)  # This will give you [  0  29  58  87 ..., 638]\n",
        "\n",
        "# Make predictions\n",
        "predicted_lstm = model.predict(X_test)\n",
        "# Select the rows at these indices\n",
        "selected_rows = predicted_lstm[indices]\n",
        "prediction_reduced = selected_rows[:, -30:]\n",
        "# Reshape the predicted and actual values to 2D (samples * time_steps, features)\n",
        "predicted_lstm = prediction_reduced.reshape(-1, 1)\n",
        "\n",
        "selected_rows = y_test[indices]\n",
        "y_test_reduced = selected_rows[:, -30:]\n",
        "y_test_actual = y_test_reduced.reshape(-1, 1)\n",
        "\n",
        "# Inverse transform the predicted and actual values\n",
        "predicted_lstm = scaler.inverse_transform(predicted_lstm)\n",
        "y_test_actual = scaler.inverse_transform(y_test_actual)\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(15, 6))\n",
        "plt.plot(y_test_actual, label='Actual')\n",
        "plt.plot(predicted_lstm, label='Predicted')\n",
        "plt.title('Actual vs Predicted')\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Price')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "jhaySJrgl2Nn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}